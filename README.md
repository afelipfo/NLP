# Advanced models for Natural Language Processing

Brief repo about advanced models for Natural Language Processing


## Course content

### Week 1

- Logistic Regression
- Neural models like feedforward
- PyTorch

### Week 2

- Sequantial models: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)
- Applications in text analysis and generation

### Week 3

- Encoder-decoder architecture in seq-2-seq tasks as automatic translation
- BPE algorithm (Byte Pair Encoding) for subwords tokenization

### Week 4

- Transformers architecture: Self-attention, multihead-attention and positional embeddings

### Week 5

- BERT (Bidirectional Encoder Representations from Transformers)
- Contextual embeddings and finetuning for entity recognition or text classification

### Week 6

- Large Language Models (LLMs) based on Transformers
- Sampling techniques for text generation

### Week 7

- Prompt Engineering
- Context learning and model alignment strategies

### Week 8

- Ethics on builing NLP solutions